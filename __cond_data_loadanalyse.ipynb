{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0921a855-374c-4b41-a132-fc6582beb95b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                <script type=\"application/javascript\" id=\"jupyter_black\">\n",
       "                (function() {\n",
       "                    if (window.IPython === undefined) {\n",
       "                        return\n",
       "                    }\n",
       "                    var msg = \"WARNING: it looks like you might have loaded \" +\n",
       "                        \"jupyter_black in a non-lab notebook with \" +\n",
       "                        \"`is_lab=True`. Please double check, and if \" +\n",
       "                        \"loading with `%load_ext` please review the README!\"\n",
       "                    console.log(msg)\n",
       "                    alert(msg)\n",
       "                })()\n",
       "                </script>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "from statistics import mean, median\n",
    "\n",
    "%load_ext jupyter_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c710661-af9a-4961-a019-7f9c9924f70c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "752900c4-ef96-46c2-aa89-355f3ee151ad",
   "metadata": {},
   "source": [
    "## Functions ‚®è"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa66747d-c573-4884-94ef-881d31038495",
   "metadata": {},
   "source": [
    "### üß©  Functions: general & data loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38890645-8418-4433-9610-f9ca20b4a9e9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Function to get pathnames ‚®èüèî"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f2adc58-67a4-497d-b707-b5c2e95a8451",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get the names of file paths\n",
    "def get_file_paths(root_folder, filename):\n",
    "    file_paths = []\n",
    "    for folder_name, subfolders, filenames in os.walk(root_folder):\n",
    "        for fname in filenames:\n",
    "            if fname == filename:\n",
    "                file_paths.append(os.path.join(folder_name, fname))\n",
    "\n",
    "    return file_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83feefdf-cf51-4722-975d-df221acce3eb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Function to extract only desired data ‚®èüõ¢ NB THIS VERSION INCLUDES UP TO 5 WORMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a8c9ca3-757e-4b23-8c98-8c777a61bcc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_extract(filepath_metadata_featuresN):\n",
    "    # print(\"filepath_metadata_featuresN: \", filepath_metadata_featuresN)\n",
    "    # import data\n",
    "\n",
    "    timedata = pd.read_hdf(filepath_metadata_featuresN, \"/timeseries_data\")\n",
    "    trajdata = pd.read_hdf(filepath_metadata_featuresN, \"/trajectories_data\")\n",
    "\n",
    "    # if there is no manual wormindex then keep wormindex from timeseries,\n",
    "    # and use was_skeletonized to put into usual \"has skeleton\" column\n",
    "    if \"worm_index_manual\" not in trajdata.columns:\n",
    "        trajectories = pd.DataFrame(\n",
    "            {\n",
    "                \"worm_id\": timedata[\"worm_index\"],\n",
    "                \"has_skeleton\": trajdata[\"was_skeletonized\"],\n",
    "                \"frame_number\": trajdata[\"frame_number\"],\n",
    "                \"x\": trajdata[\"coord_x\"],\n",
    "                \"y\": trajdata[\"coord_y\"],\n",
    "            }\n",
    "        )\n",
    "\n",
    "    # if there is a manual worm index in traj file then keep that instead of wormindex\n",
    "    else:\n",
    "        trajectories = pd.DataFrame(\n",
    "            {\n",
    "                \"worm_id\": trajdata[\"worm_index_manual\"],\n",
    "                \"has_skeleton\": trajdata[\"has_skeleton\"],\n",
    "                \"frame_number\": trajdata[\"frame_number\"],\n",
    "                \"x\": trajdata[\"coord_x\"],\n",
    "                \"y\": trajdata[\"coord_y\"],\n",
    "            }\n",
    "        )\n",
    "    trajectories = pd.concat(\n",
    "        [\n",
    "            trajectories,\n",
    "            timedata[timedata.iloc[:, 3:].dropna(axis=\"columns\", how=\"all\").columns],\n",
    "        ],\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "    # keep only the data that has good skeletons\n",
    "    trajectories = trajectories[trajectories[\"has_skeleton\"] == 1]\n",
    "\n",
    "    # from that, keep only the data corresponding to the first two worm ids, which are the 2 good ones\n",
    "    trajectories = trajectories[\n",
    "        trajectories[\"worm_id\"].isin(trajectories[\"worm_id\"].unique()[0:3])\n",
    "    ]\n",
    "\n",
    "    # from that, keep only data of those ids if they have at least 100 frames\n",
    "    frame_cutoff = 100\n",
    "    longtrajectories = (\n",
    "        pd.DataFrame()\n",
    "    )  # this will contain only the trajectories if they pass the longer than framecutoff rule\n",
    "    for i in range(\n",
    "        len((trajectories[\"worm_id\"].unique()[0:2]))\n",
    "    ):  # cycle through 0 to 1 or just 0 if only one index\n",
    "        a = trajectories[\"worm_id\"].unique()[\n",
    "            i\n",
    "        ]  # get object that is of the wormid column only those = first index\n",
    "        length_a = (trajectories[\"worm_id\"] == a).sum()  # check how long it is\n",
    "        if length_a > frame_cutoff:  # if a is greater than framecutoff then keep it\n",
    "            longtrajectories = pd.concat(\n",
    "                [\n",
    "                    longtrajectories,\n",
    "                    trajectories[\n",
    "                        trajectories[\"worm_id\"] == trajectories[\"worm_id\"].unique()[i]\n",
    "                    ],\n",
    "                ]\n",
    "            )\n",
    "\n",
    "    return longtrajectories  # which is actutruey mostly timeseries df but have inserted some things from trajectories df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89070a2-8ab6-435c-bd12-c78a8cd71377",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Function to organise loading true data ‚®èüóÑ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56116271-b349-45c1-8aee-6e8e26559e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input args: df with pathnames\n",
    "# output is\n",
    "# bigdict = {\"fpath\": {\"cond\": \"sexc\", \"data\": pd.DataFrame()}}\n",
    "# bigdf = one big data frame will true worms true conditions.\n",
    "# has added column \"cond\" (mock/avsv/sexc) and \"filename\" (corresponds to videoname)\n",
    "\n",
    "\n",
    "def organise_data(pathname_dict):\n",
    "    bigdf = pd.DataFrame()\n",
    "    bigdict = dict()\n",
    "    for key, val in pathname_dict.items():\n",
    "        for i in range(len(pathname_dict[key])):\n",
    "            # assign pathname to key in new bigdict\n",
    "            bigdict[pathname_dict[key][i]] = dict()\n",
    "            # save cond within bigdict type cond\n",
    "            bigdict[pathname_dict[key][i]][\"cond\"] = key\n",
    "            # load and save data within bigdict type data\n",
    "            bigdict[pathname_dict[key][i]][\"data\"] = import_extract(\n",
    "                pathname_dict[key][i],\n",
    "            )\n",
    "            # access dataframes within data type and create new columns for name and for cond\n",
    "            bigdict[pathname_dict[key][i]][\"data\"][\"cond\"] = key\n",
    "            bigdict[pathname_dict[key][i]][\"data\"][\"filename\"] = pathname_dict[key][i]\n",
    "\n",
    "            # now append this new dataframe into dataframe with true data\n",
    "            bigdf = pd.concat([bigdf, bigdict[pathname_dict[key][i]][\"data\"]])\n",
    "\n",
    "    return bigdf, bigdict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a7bc3e-bbe4-4f22-bfde-f3930785fe93",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Function to check consecutive numbers ‚®è 1 2 3#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38f1f48a-194f-4812-a57b-d25356febb30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def consecutive(data, stepsize=1):\n",
    "    return np.split(data, np.where(np.diff(data) != stepsize)[0] + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ec0628-f994-426a-9701-6aa9b5cf8452",
   "metadata": {},
   "source": [
    "### üîç Functions: find & clean reversals, forward runs, and omegas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249bdcdf-dd75-4f4a-87c0-9be0a1881d28",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Function to find reversals ‚®èüîé‚Ü©Ô∏è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11087fe0-8689-4c23-bc6b-3726d08d3cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def findreversals(worm_frame_speed):\n",
    "    revStartInd = []  # empty lists\n",
    "    revEndInd = []\n",
    "\n",
    "    reversals = worm_frame_speed.index[worm_frame_speed[\"speed\"] < 0]\n",
    "    # print(\"reversals: \", reversals)\n",
    "    # reversal is list of indexs of all frames with neg speed\n",
    "\n",
    "    # first get two lists with start and end idexes of reversals\n",
    "    # then will check within reversal, if there is jump in frame split in two reversals,\n",
    "    # if there are less than 15 nans and neg then count as one\n",
    "\n",
    "    # go through each element of reversals ie all the indexes with neg speed\n",
    "    # and pull out start and end indices\n",
    "    for r in reversals:\n",
    "        # print(\"r is\", r)\n",
    "        #### now start going through and checking\n",
    "        # count first revindex as start of reversal,\n",
    "        # if next is pos/nan also count as end of reversal\n",
    "        if r == np.min(reversals):\n",
    "            revStartInd.append(r)\n",
    "            if worm_frame_speed.loc[r + 1][\"speed\"] >= 0 or pd.isna(\n",
    "                worm_frame_speed.loc[r + 1][\"speed\"]\n",
    "            ):\n",
    "                revEndInd.append(r)\n",
    "                # print(\"appended\", r, \"to revendind\")\n",
    "\n",
    "        # if the index is not 0 and is not last check whether previous&last indexs have positive speed\n",
    "        elif (r > np.min(reversals)) & (r < np.max(reversals)):\n",
    "            #####CHECK PREV ROW#####\n",
    "            # if speed in prev row is greater than 0\n",
    "            if worm_frame_speed.loc[r - 1][\"speed\"] >= 0 or pd.isna(\n",
    "                worm_frame_speed.loc[r - 1][\"speed\"]\n",
    "            ):\n",
    "                revStartInd.append(r)\n",
    "                # print(\"appended\", r, \"to revstartind\")\n",
    "\n",
    "            #####CHECK NEXT ROW#####\n",
    "            # if speed in next row is greater than 0 or nan\n",
    "            if worm_frame_speed.loc[r + 1][\"speed\"] >= 0 or pd.isna(\n",
    "                worm_frame_speed.loc[r + 1][\"speed\"]\n",
    "            ):\n",
    "                revEndInd.append(r)\n",
    "                # print(\"appended\", r, \"to revendind\")\n",
    "\n",
    "        # if neg speed at last negative speed index count it as end of last reversal\n",
    "        # if prev was positive/nan count as start of reversal\n",
    "        elif r == np.max(reversals):\n",
    "            revEndInd.append(r)\n",
    "\n",
    "            if worm_frame_speed.loc[r - 1][\"speed\"] >= 0 or pd.isna(\n",
    "                worm_frame_speed.loc[r - 1][\"speed\"]\n",
    "            ):\n",
    "                revStartInd.append(r)\n",
    "                # print(\"appended\", r, \"to revstartind\")\n",
    "\n",
    "    # check same number of rev start and end\n",
    "    if len(revStartInd) != len(revEndInd):\n",
    "        print(\"PROBLEM: revstart and revend indices don't match up :O find rev\")\n",
    "\n",
    "    return revStartInd, revEndInd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecad82db-294e-4da9-b695-5981a898b117",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Function to clean reversals + calculate length ‚®èüõÅ‚Ü©Ô∏è + üìè‚Ü©Ô∏è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "603b1480-5b8d-45dc-83ee-a1b715e169b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now clean reversal start/end indices and calculate reversal lengths\n",
    "def cleanreversals(worm_frame_speed, revStartInd, revEndInd):\n",
    "    # create empty lists to hold output\n",
    "    clean_revStartInd = []\n",
    "    clean_revEndInd = []\n",
    "\n",
    "    # first check frames within this reversal\n",
    "    for l in range(len(revStartInd)):\n",
    "        ##first assess whether there are any frames missing in this reversal\n",
    "        # print(\"l\", l)\n",
    "        # print(\"len revEndInd\", len(revEndInd))\n",
    "        # print(\"len revStartInd\", len(revStartInd))\n",
    "        # length in index counts of this reversal\n",
    "        this_revlenI = revEndInd[l] - revStartInd[l]\n",
    "\n",
    "        # print(\"revEndInd[l]\", revEndInd[l])\n",
    "        # print(\"revStartInd[l]\", revStartInd[l])\n",
    "\n",
    "        # length in frames of this reversal\n",
    "        this_revlenF = (\n",
    "            worm_frame_speed.loc[revEndInd[l], \"frame_number\"]\n",
    "            - worm_frame_speed.loc[revStartInd[l], \"frame_number\"]\n",
    "        )\n",
    "\n",
    "        # append start and end indices of reversal to clean dfs, then see if need to add any\n",
    "        clean_revStartInd.append(revStartInd[l])\n",
    "        clean_revEndInd.append(revEndInd[l])\n",
    "\n",
    "        # if frame and index count dont match then check how many frames missing and if consecutive\n",
    "        if this_revlenI != this_revlenF:\n",
    "            # do consec analysis\n",
    "            # get list of frame numbers between those two indexes\n",
    "            this_rev_frames = worm_frame_speed.loc[\n",
    "                revStartInd[l] : revEndInd[l], \"frame_number\"\n",
    "            ].tolist()\n",
    "            rev_cc_array = consecutive(this_rev_frames)\n",
    "\n",
    "            for i in range(len(rev_cc_array)):\n",
    "                # print(\"i is cycle inside len of cc array\", i)\n",
    "                # print(\"rev_cc_array[i]\", rev_cc_array[i])\n",
    "                # start at i=1 because first reversal break happens between 0 and 1\n",
    "                if i > 0:\n",
    "                    revbreakend = rev_cc_array[i][0]\n",
    "                    revbreakstart = rev_cc_array[i - 1][-1]\n",
    "                    revbreaklen = revbreakend - revbreakstart\n",
    "                    # if reversal breakend is greater than max_frames_missing, append the frame of where rev break starts to be new revend ind\n",
    "                    # and frame of where rev break ends to be new revstart ind\n",
    "                    if revbreaklen > max_frames_missing + 1:\n",
    "                        clean_revEndInd.append(\n",
    "                            worm_frame_speed[\n",
    "                                worm_frame_speed[\"frame_number\"] == revbreakstart\n",
    "                            ].index[0]\n",
    "                        )\n",
    "                        clean_revStartInd.append(\n",
    "                            worm_frame_speed[\n",
    "                                worm_frame_speed[\"frame_number\"] == revbreakend\n",
    "                            ].index[0]\n",
    "                        )\n",
    "\n",
    "    # now check frames between reversals\n",
    "    for l in range(len(revStartInd)):\n",
    "        # dont need to check what came before first reversal, is definitely rev start\n",
    "        # if is 2nd revstart or later, check interval between previous rev end and this\n",
    "        # want only frames that are in interval hence +1 and -1 after the revendind and revstartind\n",
    "        if l > 0:\n",
    "            # pull out interval of reversal (includes revstart and revend)\n",
    "            this_interval = worm_frame_speed.loc[\n",
    "                (revEndInd[l - 1]) : (revStartInd[l]), [\"frame_number\", \"speed\"]\n",
    "            ]\n",
    "            this_intervalS = this_interval[\"speed\"]\n",
    "            # get the the number of frames between last revend and this revstart\n",
    "            this_intervalF = this_interval[\"frame_number\"]\n",
    "            this_intervalF = this_intervalF.iloc[-1] - this_intervalF.iloc[0]\n",
    "            # if interval between reversals is all nans AND 4 frames or less\n",
    "            if all(this_intervalS.iloc[1:-1].isna()) and this_intervalF <= (\n",
    "                max_frames_missing + 1\n",
    "            ):\n",
    "                # then get rid of revEndInd[l - 1] and revStartInd[l]\n",
    "                clean_revStartInd.remove(revStartInd[l])\n",
    "                clean_revEndInd.remove(revEndInd[l - 1])\n",
    "\n",
    "    # sort lists to make sure added frames are true in order\n",
    "    clean_revStartInd.sort()\n",
    "    clean_revEndInd.sort()\n",
    "\n",
    "    # check same number of rev start and end\n",
    "    if len(clean_revStartInd) != len(clean_revEndInd):\n",
    "        print(\"PROBLEM: revstart and revend indices don't match up :O\")\n",
    "\n",
    "    # calculate reversal length\n",
    "    revstart_frames = worm_frame_speed.loc[clean_revStartInd, \"frame_number\"].tolist()\n",
    "    revend_frames = worm_frame_speed.loc[clean_revEndInd, \"frame_number\"].tolist()\n",
    "    rev_lengths = np.array(revend_frames) - np.array(revstart_frames)\n",
    "    rev_lengths += 1\n",
    "    rev_lengths = rev_lengths.tolist()\n",
    "\n",
    "    # print(\"revStartInd\", revStartInd)\n",
    "    # print(\"revEndInd\", revEndInd)\n",
    "    # print(\"clean_revStartInd\", clean_revStartInd)\n",
    "    # print(\"clean_revEndInd\", clean_revEndInd)\n",
    "    # print(\"rev_lengths\", rev_lengths)\n",
    "\n",
    "    return clean_revStartInd, clean_revEndInd, rev_lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc090f7-394a-4061-ac77-b4d50ae3f9d4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Function to find fwd runs ‚®èüîé‚¨ÜÔ∏è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "05f207d3-a8e8-4fce-bc77-7e2c59025cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def findfwdruns(worm_frame_speed):\n",
    "    # empty lists\n",
    "    fwdStartInd = []\n",
    "    fwdEndInd = []\n",
    "\n",
    "    # NB forwards contains all indexes with positive or 0 speed, whereas reversals contains only indexes with neg speed\n",
    "    forwards = worm_frame_speed.index[worm_frame_speed[\"speed\"] >= 0]\n",
    "\n",
    "    # print(\"forwards: \", forwards)\n",
    "    # forwards is list of indexes of all frames with neg speed\n",
    "\n",
    "    # first get two lists with start and end idexes of forward runs\n",
    "    # then will check within forward run, if there is jump in frame split in two forward runs,\n",
    "    # if there are less than 15 nans and pos speed again then count as one\n",
    "\n",
    "    # go through each element of forwards ie all the indexes with pos or 0 speed\n",
    "    # and pull out start and end indices\n",
    "\n",
    "    for r in forwards:\n",
    "        # print(\"r is\", r)\n",
    "        #### now start going through and checking\n",
    "        # count first fwdindex as start of fwdrun,\n",
    "        # then check if there is a next - could be first and last, in which case append also to endind\n",
    "        # if there is next and is neg/nan also count as end of fwdrun (dont count as end of fwd run if its 0)\n",
    "        if r == np.min(forwards):\n",
    "            fwdStartInd.append(r)\n",
    "            # print(\"appended\", r, \"to fwdstartind\")\n",
    "\n",
    "            if r == np.max(forwards):\n",
    "                fwdEndInd.append(r)\n",
    "            elif r < np.max(forwards):\n",
    "                if worm_frame_speed.loc[r + 1][\"speed\"] < 0 or pd.isna(\n",
    "                    worm_frame_speed.loc[r + 1][\"speed\"]\n",
    "                ):\n",
    "                    fwdEndInd.append(r)\n",
    "                    # print(\"appended\", r, \"to fwdendind\")\n",
    "\n",
    "        # if the index is not 0 and is not last check whether previous&last indexs have negative speed\n",
    "        elif (r > np.min(forwards)) & (r < np.max(forwards)):\n",
    "            #####CHECK PREV ROW#####\n",
    "            # if speed in prev row is smaller than 0 or nan\n",
    "            if worm_frame_speed.loc[r - 1][\"speed\"] < 0 or pd.isna(\n",
    "                worm_frame_speed.loc[r - 1][\"speed\"]\n",
    "            ):\n",
    "                fwdStartInd.append(r)\n",
    "                # print(\"appended\", r, \"to fwdstartind\")\n",
    "\n",
    "            #####CHECK NEXT ROW#####\n",
    "            # if speed in next row is smaller than 0 or nan\n",
    "            if worm_frame_speed.loc[r + 1][\"speed\"] < 0 or pd.isna(\n",
    "                worm_frame_speed.loc[r + 1][\"speed\"]\n",
    "            ):\n",
    "                fwdEndInd.append(r)\n",
    "                # print(\"appended\", r, \"to fwdendind\")\n",
    "\n",
    "        # if pos speed at last forward speed index, count it as end of last fwd run\n",
    "        # if prev was negative/nan count also as start of fwd run\n",
    "        elif r == np.max(forwards):\n",
    "            fwdEndInd.append(r)\n",
    "            # print(\"appended\", r, \"to fwdendind\")\n",
    "\n",
    "            if worm_frame_speed.loc[r - 1][\"speed\"] < 0 or pd.isna(\n",
    "                worm_frame_speed.loc[r - 1][\"speed\"]\n",
    "            ):\n",
    "                fwdStartInd.append(r)\n",
    "                # print(\"appended\", r, \"to fwdstartind\")\n",
    "\n",
    "    # check same number of rev start and end\n",
    "    if len(fwdStartInd) != len(fwdEndInd):\n",
    "        print(\n",
    "            \"PROBLEM: fwdstart and fwdend indices don't match up :O (find fwdruns function)\"\n",
    "        )\n",
    "\n",
    "    # print(\"fwdStartInd\", fwdStartInd)\n",
    "    # print(\"fwdEndInd\", fwdEndInd)\n",
    "\n",
    "    return fwdStartInd, fwdEndInd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e99194d-7c2b-4783-8600-788fd97463a8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Function to clean fwd runs + calculate length ‚®èüõÅ‚¨ÜÔ∏è + üìè‚¨Ü"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "542758df-8664-46f1-a78e-346f5088ae55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean forward run start/end indices and calculate fwdrun lengths\n",
    "def cleanfwdruns(worm_frame_speed, fwdStartInd, fwdEndInd):\n",
    "    # create empty lists to hold output\n",
    "    clean_fwdStartInd = []\n",
    "    clean_fwdEndInd = []\n",
    "\n",
    "    # first check frames within this fwd run\n",
    "    for l in range(len(fwdStartInd)):\n",
    "        ##first assess whether there are any frames missing in this fwdrun\n",
    "        # print(\"l\", l)\n",
    "        # print(\"len fwdEndInd\", len(fwdEndInd))\n",
    "        # print(\"len fwdStartInd\", len(fwdStartInd))\n",
    "        # length in index counts of this fwdrun\n",
    "        this_fwdlenI = fwdEndInd[l] - fwdStartInd[l]\n",
    "\n",
    "        # print(\"fwdEndInd[l]\", fwdEndInd[l])\n",
    "        # print(\"fwdStartInd[l]\", fwdStartInd[l])\n",
    "\n",
    "        # length in frames of this fwd run\n",
    "        this_fwdlenF = (\n",
    "            worm_frame_speed.loc[fwdEndInd[l], \"frame_number\"]\n",
    "            - worm_frame_speed.loc[fwdStartInd[l], \"frame_number\"]\n",
    "        )\n",
    "\n",
    "        # append start and end indices of fwd run to clean dfs, then see if need to add any\n",
    "        clean_fwdStartInd.append(fwdStartInd[l])\n",
    "        clean_fwdEndInd.append(fwdEndInd[l])\n",
    "\n",
    "        # if frame and index count dont match then check how many frames missing and if consecutive\n",
    "        if this_fwdlenI != this_fwdlenF:\n",
    "            # do consecutive analysis\n",
    "            # get list of frame numbers between those two indexes\n",
    "            this_fwd_frames = worm_frame_speed.loc[\n",
    "                fwdStartInd[l] : fwdEndInd[l], \"frame_number\"\n",
    "            ].tolist()\n",
    "            fwd_cc_array = consecutive(this_fwd_frames)\n",
    "\n",
    "            for i in range(len(fwd_cc_array)):\n",
    "                # print(\"i is cycle inside len of cc array\", i)\n",
    "                # print(\"fwd_cc_array[i]\", fwd_cc_array[i])\n",
    "                # start at i=1 because first fwdrun break happens between 0 and 1\n",
    "                if i > 0:\n",
    "                    fwdbreakend = fwd_cc_array[i][0]\n",
    "                    fwdbreakstart = fwd_cc_array[i - 1][-1]\n",
    "                    fwdbreaklen = fwdbreakend - fwdbreakstart\n",
    "                    # if fwd run breakend is greater than max_frames_missing, append the frame of where fwd break starts to be new fwdend ind\n",
    "                    # and frame of where fwd break ends to be new fwdstart ind\n",
    "                    if fwdbreaklen > max_frames_missing + 1:\n",
    "                        clean_fwdEndInd.append(\n",
    "                            worm_frame_speed[\n",
    "                                worm_frame_speed[\"frame_number\"] == fwdbreakstart\n",
    "                            ].index[0]\n",
    "                        )\n",
    "                        clean_fwdStartInd.append(\n",
    "                            worm_frame_speed[\n",
    "                                worm_frame_speed[\"frame_number\"] == fwdbreakend\n",
    "                            ].index[0]\n",
    "                        )\n",
    "\n",
    "    # now check frames between fwd runs\n",
    "    for l in range(len(fwdStartInd)):\n",
    "        # dont need to check what came before first fwd run, is definitely fwd start\n",
    "        # if is 2nd fwd run or later, check interval between previous fwd end and this fwd start\n",
    "        # want only frames that are in interval hence -1 after the fwdendind\n",
    "        if l > 0:\n",
    "            # pull out interval of fwd run (includes fwdstart and fwdend)\n",
    "            this_interval = worm_frame_speed.loc[\n",
    "                (fwdEndInd[l - 1]) : (fwdStartInd[l]), [\"frame_number\", \"speed\"]\n",
    "            ]\n",
    "            this_intervalS = this_interval[\"speed\"]\n",
    "            # get the the number of frames between last fwdend and this fwdstart\n",
    "            this_intervalF = this_interval[\"frame_number\"]\n",
    "            this_intervalF = this_intervalF.iloc[-1] - this_intervalF.iloc[0]\n",
    "            # if interval between fwd runs is all nans AND 4 frames or less\n",
    "            if all(this_intervalS.iloc[1:-1].isna()) and this_intervalF <= (\n",
    "                max_frames_missing + 1\n",
    "            ):\n",
    "                # then get rid of fwdEndInd[l - 1] and fwdStartInd[l]\n",
    "                clean_fwdStartInd.remove(fwdStartInd[l])\n",
    "                clean_fwdEndInd.remove(fwdEndInd[l - 1])\n",
    "\n",
    "    # sort lists to make sure added frames are all in order\n",
    "    clean_fwdStartInd.sort()\n",
    "    clean_fwdEndInd.sort()\n",
    "\n",
    "    # check same number of fwd start and end\n",
    "    if len(clean_fwdStartInd) != len(clean_fwdEndInd):\n",
    "        print(\"PROBLEM: fwdstart and fwdend indices don't match up :O\")\n",
    "\n",
    "    # calculate fwd run length\n",
    "    fwdstart_frames = worm_frame_speed.loc[clean_fwdStartInd, \"frame_number\"].tolist()\n",
    "    fwdend_frames = worm_frame_speed.loc[clean_fwdEndInd, \"frame_number\"].tolist()\n",
    "    fwd_lengths = np.array(fwdend_frames) - np.array(fwdstart_frames)\n",
    "    fwd_lengths += 1\n",
    "    fwd_lengths = fwd_lengths.tolist()\n",
    "\n",
    "    # print(\"fwdStartInd\", fwdStartInd)\n",
    "    # print(\"fwdEndInd\", fwdEndInd)\n",
    "    # print(\"clean_fwdStartInd\", clean_fwdStartInd)\n",
    "    # print(\"clean_fwdEndInd\", clean_fwdEndInd)\n",
    "    # print(\"fwd_lengths\", fwd_lengths)\n",
    "\n",
    "    return clean_fwdStartInd, clean_fwdEndInd, fwd_lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce63c37e-31fe-41ea-bac9-72d7119e666a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### function to remove reversals too short or too long üö´‚Ü©Ô∏èüë∂üèªüë®üèº‚Äçü¶≥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a7f9869b-19c2-44c5-97c6-668a540f12d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# min and max length for considering as reversals\n",
    "# minrevlen = 15\n",
    "# maxrevlen = 80\n",
    "\n",
    "\n",
    "def exclude_short_long_revs(data_df):\n",
    "    # make copy of data_df to work on within function\n",
    "    infunct_data_df = data_df.copy()\n",
    "\n",
    "    # add allrevstart and allrevend columns to infunct_data_df\n",
    "    infunct_data_df[\"TrueRevStart\"] = False\n",
    "    infunct_data_df[\"TrueRevEnd\"] = False\n",
    "\n",
    "    # will contain rev len lists and unique id list matching length to create new true_rev_lengths\n",
    "    revlengs_ids = []\n",
    "    revlengs = []\n",
    "    true_rev_lengths = []\n",
    "\n",
    "    wormlist = infunct_data_df[\"unique_id\"].unique()\n",
    "\n",
    "    # cycle within each worm\n",
    "    for worm in wormlist:\n",
    "        # list to hold lists of true reversal lengths\n",
    "        this_worm_tRLg = []\n",
    "\n",
    "        this_worm = infunct_data_df[infunct_data_df[\"unique_id\"] == worm].copy()\n",
    "\n",
    "        # get list with revstart and revend indices = true\n",
    "        revstarts = this_worm[this_worm[\"RevStart\"] == True].index.tolist()\n",
    "        revends = this_worm[this_worm[\"RevEnd\"] == True].index.tolist()\n",
    "\n",
    "        # cycle within the list of revstart = true indices\n",
    "        for R in range(len(revstarts)):\n",
    "            # for each revstart, check how many frames between revstart and next revend\n",
    "            # R is position in list of revstarts\n",
    "            # fstart is start and end frames of\n",
    "            fstart = this_worm.loc[revstarts[R], \"frame_number\"]\n",
    "            fend = this_worm.loc[revends[R], \"frame_number\"]\n",
    "            this_rev_len = fend - fstart + 1\n",
    "\n",
    "            # if 15<this_rev_len<80\n",
    "            # add revstart and revend to truerevstart and truerevend columns\n",
    "            # add revlen to new_rev_lengths\n",
    "            if this_rev_len > minrevlen and this_rev_len < maxrevlen:\n",
    "                infunct_data_df.loc[revstarts[R], \"TrueRevStart\"] = True\n",
    "                infunct_data_df.loc[revends[R], \"TrueRevEnd\"] = True\n",
    "\n",
    "                this_worm_tRLg.append(this_rev_len)\n",
    "\n",
    "        # to store reversal lengths\n",
    "        # cycle thrugh list of reversal lengths and append worm id to worm id list and rev leng to rev list\n",
    "        # then will join both these lists into df\n",
    "        for k in this_worm_tRLg:\n",
    "            revlengs_ids.append(worm)\n",
    "            revlengs.append(k)\n",
    "\n",
    "    infunct_data_df.rename(\n",
    "        columns={\n",
    "            \"RevStart\": \"ogRevStart\",\n",
    "            \"RevEnd\": \"ogRevEnd\",\n",
    "            \"TrueRevStart\": \"RevStart\",\n",
    "            \"TrueRevEnd\": \"RevEnd\",\n",
    "        },\n",
    "        inplace=True,\n",
    "    )\n",
    "\n",
    "    # now create true_rev_lengths\n",
    "    # thisdf will contain unique worm id (short) as 1st column and the length of each reversal in 2nd column\n",
    "    true_rev_lengths = pd.DataFrame()\n",
    "    true_rev_lengths = true_rev_lengths.assign(unique_id=revlengs_ids, revlen=revlengs)\n",
    "\n",
    "    return infunct_data_df, true_rev_lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92915b95-5249-4c0e-a131-2f59739e205e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### function to correct misidentified heads in reversals ‚®èüîÑüë∂üèªüêõ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ddacf7d5-bbd4-4333-9f3f-fe1724d68356",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_misheads(corrected_data_df, head_errors_list):\n",
    "    # add this column which will show whether worm has had head corrected or not\n",
    "    corrected_data_df[\"mishead\"] = False\n",
    "\n",
    "    for index, row in head_errors_list.iterrows():\n",
    "        # get some values specific to this worm\n",
    "        this_worm = head_errors_list.loc[index, \"unique_id_long\"]\n",
    "        print(\"this_worm ie long\", this_worm)\n",
    "        print(\"unique id short\", head_errors_list.loc[index, \"unique_id\"])\n",
    "        this_SF = head_errors_list.loc[index, \"StartFrame\"]\n",
    "        this_EF = head_errors_list.loc[index, \"EndFrame\"]\n",
    "        # find start and end index of the stretch to be corrected\n",
    "        startindex = corrected_data_df[\n",
    "            (corrected_data_df[\"unique_id_long\"] == this_worm)\n",
    "            & (corrected_data_df[\"frame_number\"] == this_SF)\n",
    "        ].index[0]\n",
    "        endindex = corrected_data_df[\n",
    "            (corrected_data_df[\"unique_id_long\"] == this_worm)\n",
    "            & (corrected_data_df[\"frame_number\"] == this_EF)\n",
    "        ].index[0]\n",
    "\n",
    "        # select this stretch in dataframe and replace with inverse values\n",
    "        corrected_data_df.loc[startindex:endindex, \"speed\"] = -corrected_data_df.loc[\n",
    "            startindex:endindex, \"speed\"\n",
    "        ]\n",
    "\n",
    "        # change value of \"mishead\" column to true to indicate this worm has had its head misidentified and thus speed corrected\n",
    "        corrected_data_df.loc[\n",
    "            corrected_data_df[\"unique_id\"] == this_worm, \"mishead\"\n",
    "        ] = True\n",
    "\n",
    "        print(\n",
    "            \"unique id short from new dataset\",\n",
    "            corrected_data_df.loc[\n",
    "                (corrected_data_df[\"unique_id_long\"] == this_worm)\n",
    "                & (corrected_data_df[\"frame_number\"] == this_SF),\n",
    "                \"unique_id\",\n",
    "            ],\n",
    "        )\n",
    "\n",
    "    return corrected_data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad94739-aba7-4284-9209-964f308b3980",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### function to find (inferred) omega turns ‚®èŒ©‚Ü∫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5033c821-a5c8-4d5f-9594-5529beb0a31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "minomegalength = 20\n",
    "maxomegalength = 400\n",
    "\n",
    "# draft find omega turns function\n",
    "\n",
    "\n",
    "def findomegas(data_df):\n",
    "    # NB omega start frame is last frame of rev, omega end frame is next frame in worm (cant label missing frame!)\n",
    "\n",
    "    # create 2 new columns in datadf: omegaS, omegaE (all false)\n",
    "    data_df[\"OmegaS\"] = False\n",
    "    data_df[\"OmegaE\"] = False\n",
    "\n",
    "    # create 4 lists: omega_startindex, omegalengths, revlengths (nb revlenghts contains only the lenght of reversals followed by omega), unique_id\n",
    "    omega_startindex = []\n",
    "    omega_startframes = []\n",
    "    omega_lengths = []\n",
    "    omega_revlengths = []\n",
    "    omega_ids = []\n",
    "\n",
    "    # cycle through each worm\n",
    "    worms = data_df[\"unique_id\"].unique().tolist()\n",
    "    for worm in worms:\n",
    "        # print(\"worm\", worm)\n",
    "        this_worm = data_df[data_df[\"unique_id\"] == worm]\n",
    "\n",
    "        # within worm, pull out two lists of framenumber at which ogrevstart = true, and ogrevend = true\n",
    "        thiswormRS = this_worm.index[this_worm[\"ogRevStart\"] == True].tolist()\n",
    "        thiswormRE = this_worm.index[this_worm[\"ogRevEnd\"] == True].tolist()\n",
    "\n",
    "        # print(\"thiswormRS\", thiswormRS)\n",
    "        # print(\"thiswormRE\", thiswormRE)\n",
    "\n",
    "        # save also the last index corresponding to this worm\n",
    "        thisworm_maxindex = this_worm.index.max()\n",
    "\n",
    "        # then cycle through list of OGREVEND frames\n",
    "        for RE in thiswormRE:\n",
    "            # print(\"RE\", RE)\n",
    "            # save frame number\n",
    "            REframenumber = this_worm.loc[RE, \"frame_number\"]\n",
    "\n",
    "            # save remainder of worm speed as smaller df\n",
    "            speedseries = this_worm.loc[(RE + 1) : thisworm_maxindex, [\"speed\"]]\n",
    "            # print(\"speedseries\", speedseries)\n",
    "\n",
    "            # check if speedseries is empty (this happens if reversal ends on last frame of that worm). only do the rest if is not empty (IE False)\n",
    "            if speedseries.empty == False:\n",
    "                # print(\"speedseries is not empty\")\n",
    "                # check if next first valid index within speedseries (ie next speed that is not nan or missing) is nan\n",
    "                # if first valid index isnan is False, that means that there is a next non-nan value before end of worm\n",
    "                if pd.isna(speedseries.first_valid_index()) == False:\n",
    "                    # get (frame number of that index) - 1 (we want frame of last nan / missing index )\n",
    "                    nextvalidindex = speedseries.first_valid_index()\n",
    "                    # print(\"next valid index exists\")\n",
    "\n",
    "                # if first valid index isnan is True, that means that there is not a non-nan value before end of worm\n",
    "                # means that na stretch runs up until the end - take last frame of series as the last frame of omega turn\n",
    "                elif pd.isna(speedseries.first_valid_index()) == True:\n",
    "                    nextvalidindex = speedseries.index.max()\n",
    "                    # print(\"there is no next valid index before worm end\")\n",
    "\n",
    "                # print(\"nextvalidindex\", nextvalidindex)\n",
    "\n",
    "                # get omega start and end framenumber & index\n",
    "                # (omega starts at last frame of  rev )\n",
    "                omegastartframe = this_worm.loc[(RE), \"frame_number\"]\n",
    "                omegastartindex = RE\n",
    "\n",
    "                omegaendframe = this_worm.loc[(nextvalidindex), \"frame_number\"]\n",
    "                omegaendindex = nextvalidindex\n",
    "\n",
    "                # print(\"omegastartframe\", omegastartframe)\n",
    "                # print(\"omegaendframe\", omegaendframe)\n",
    "\n",
    "                # omegalength is not plus one bc we have taken start frame to be last frame of rev\n",
    "                omegalength = omegaendframe - omegastartframe\n",
    "                # print(\"omegalength\", omegalength)\n",
    "\n",
    "                # if omegalength is between 20-400 we consider an omega turn (try then with 80)\n",
    "                if omegalength >= minomegalength and omegalength <= maxomegalength:\n",
    "                    # add start and end trues to omegaS and omegaE\n",
    "                    data_df.loc[omegastartindex, \"OmegaS\"] = True\n",
    "                    data_df.loc[omegaendindex, \"OmegaE\"] = True\n",
    "\n",
    "                    # calculate length of preceeding reversal, from ogrevstart & ogrevend frame got at beggining\n",
    "                    thisrevnumber = thiswormRE.index(RE)\n",
    "                    thisrevstartI = thiswormRS[thisrevnumber]\n",
    "                    thisrevendI = thiswormRE[thisrevnumber]\n",
    "                    thisrevstartF = this_worm.loc[thisrevstartI, \"frame_number\"]\n",
    "                    thisrevsendF = this_worm.loc[thisrevendI, \"frame_number\"]\n",
    "\n",
    "                    thisrevlen = thisrevsendF - thisrevstartF + 1\n",
    "\n",
    "                    # add omega starti, omega length,  revlength and wormid of this omega turn to corresponding 4 lists\n",
    "                    omega_startindex.append(omegastartindex)\n",
    "                    omega_startframes.append(omegastartframe)\n",
    "                    omega_lengths.append(omegalength)\n",
    "                    omega_revlengths.append(thisrevlen)\n",
    "                    omega_ids.append(worm)\n",
    "\n",
    "            # else:\n",
    "            # print(\"speedseries is empty\")\n",
    "\n",
    "    omegas = pd.DataFrame(\n",
    "        list(\n",
    "            zip(\n",
    "                omega_ids,\n",
    "                omega_lengths,\n",
    "                omega_revlengths,\n",
    "                omega_startindex,\n",
    "                omega_startframes,\n",
    "            )\n",
    "        ),\n",
    "        columns=[\n",
    "            \"unique_id\",\n",
    "            \"omega_length\",\n",
    "            \"omega_revlength\",\n",
    "            \"omega_startindex\",\n",
    "            \"omega_startframe\",\n",
    "        ],\n",
    "    )\n",
    "    # print(\"omega_lengths\", omega_lengths)\n",
    "    # print(\"omega_revlengths\", omega_revlengths)\n",
    "    # print(\"omega_ids\", omega_ids)\n",
    "\n",
    "    return omegas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782c1401-46c6-47ed-9875-582b68029e83",
   "metadata": {},
   "source": [
    "### üî™Functions: data cleaning (general)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ecfcaf8-2b69-447b-a58a-60123fd2b833",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### function to remove worms with scant frames ‚®èüö´üí©üêõ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f5190e-d7ce-4e37-b423-475e74d51615",
   "metadata": {},
   "source": [
    "### üî™Functions: data cleaning (general)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776fde73-5dc2-405b-8845-1bd11bbd4cd9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### function to remove worms with scant frames ‚®èüö´üí©üêõ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c718dad6-94e1-4468-8fca-b32374c50076",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_shitty_worms(data_df):\n",
    "    wormlist = data_df[\"unique_id\"].unique()\n",
    "    max_frames = data_df[\"frame_number\"].max()\n",
    "    good_worms = []\n",
    "    for worm in wormlist:\n",
    "        this_worm_speed = data_df[\"speed\"][data_df[\"unique_id\"] == worm]\n",
    "        this_worm_gf = this_worm_speed.count()\n",
    "        this_worm_gf_percent = (this_worm_gf / max_frames) * 100\n",
    "        if this_worm_gf_percent > min_good_frames_percent:\n",
    "            good_worms.append(worm)\n",
    "\n",
    "    updated_data_df = data_df[data_df[\"unique_id\"].isin(good_worms)].copy()\n",
    "\n",
    "    return updated_data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891a2022-a484-4bcf-add1-b0299f846c8c",
   "metadata": {},
   "source": [
    "### ü§ìFunctions: extract data parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75794b34-e200-4562-ae57-8d6876f9c199",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### function to count good frames in each worm ‚®è‚úÖüéû"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aaa7a938-1d12-4ea8-b875-cd1dbee0e40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to check missing data\n",
    "# input is data_df\n",
    "# output is df with uniqueid, total frames that are not nan, percentage, and color code\n",
    "\n",
    "\n",
    "def count_good_frames(data_df):\n",
    "    gf_per_worm = pd.DataFrame(pd.Series(wormlist, name=\"unique_id\"))\n",
    "    gf_per_worm[\"good_frames\"] = pd.NA\n",
    "\n",
    "    # add column with color corresponding to condition (for plotting)\n",
    "    # create lists with unique_ids of each worm condition\n",
    "    mock_worms = data_df[\"unique_id\"][data_df[\"cond\"] == \"mock\"].unique().tolist()\n",
    "    avsv_worms = data_df[\"unique_id\"][data_df[\"cond\"] == \"avsv\"].unique().tolist()\n",
    "\n",
    "    for worm in wormlist:\n",
    "        this_worm_speed = data_df[\"speed\"][data_df[\"unique_id\"] == worm]\n",
    "        this_worm_gf = this_worm_speed.count()\n",
    "        gf_per_worm.loc[gf_per_worm[\"unique_id\"] == worm, \"good_frames\"] = this_worm_gf\n",
    "\n",
    "        if worm in mock_worms:\n",
    "            gf_per_worm.loc[gf_per_worm[\"unique_id\"] == worm, \"color_cond\"] = \"#a8a9a8\"\n",
    "            gf_per_worm.loc[gf_per_worm[\"unique_id\"] == worm, \"cond\"] = \"mock\"\n",
    "\n",
    "        elif worm in avsv_worms:\n",
    "            gf_per_worm.loc[gf_per_worm[\"unique_id\"] == worm, \"color_cond\"] = \"#87da4b\"\n",
    "            gf_per_worm.loc[gf_per_worm[\"unique_id\"] == worm, \"cond\"] = \"avsv\"\n",
    "\n",
    "    # add column with percentage of good frames per worm\n",
    "    max_frames = data_df[\"frame_number\"].max()\n",
    "    gf_per_worm[\"percent_good\"] = (gf_per_worm[\"good_frames\"] / max_frames) * 100\n",
    "    gf_per_worm = gf_per_worm.reindex(\n",
    "        columns=[\"color_cond\", \"cond\", \"unique_id\", \"good_frames\", \"percent_good\"]\n",
    "    )\n",
    "\n",
    "    return gf_per_worm\n",
    "    # gf_per_worm is dataframe with 4 cols:\n",
    "    # unique_id,\n",
    "    # number of good frames that worm has,\n",
    "    # percentage of good frames (frames / max frames across true conds),\n",
    "    # color code corresponding to condition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ef572a-cada-4412-b6b6-084c2c5b2ebf",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### function to calculate feats. percentage per worm: FWD and BCK movement ‚®è‚Ü©Ô∏èüêõ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ef1c0b15-9529-4581-89b3-4bcb3bc3ac11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_fbpercent(gf_per_worm, data_df):\n",
    "    fwdback_perworm = gf_per_worm.copy()\n",
    "    fwdback_perworm[\"fwdframes\"] = 0\n",
    "    fwdback_perworm[\"bckframes\"] = 0\n",
    "    fwdback_perworm[\"totalframes\"] = 0\n",
    "\n",
    "    for worm in wormlist:\n",
    "        this_worm = data_df[data_df[\"unique_id\"] == worm]\n",
    "        fwdback_perworm.loc[worm, \"fwdframes\"] = len(this_worm[this_worm[\"fw\"] == True])\n",
    "        fwdback_perworm.loc[worm, \"bckframes\"] = len(this_worm[this_worm[\"bk\"] == True])\n",
    "        fwdback_perworm.loc[worm, \"totalframes\"] = (\n",
    "            fwdback_perworm.loc[worm, \"fwdframes\"]\n",
    "            + fwdback_perworm.loc[worm, \"bckframes\"]\n",
    "        )\n",
    "\n",
    "    fwdback_perworm[\"percent_fw\"] = (\n",
    "        fwdback_perworm[\"fwdframes\"] / fwdback_perworm[\"totalframes\"]\n",
    "    ) * 100\n",
    "    fwdback_perworm[\"percent_bk\"] = (\n",
    "        fwdback_perworm[\"bckframes\"] / fwdback_perworm[\"totalframes\"]\n",
    "    ) * 100\n",
    "\n",
    "    return fwdback_perworm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb96b14e-e6ac-4bfc-94f0-dacb1a1dca74",
   "metadata": {},
   "source": [
    "### üé® Functions: plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f97b13-ec81-48ab-9035-d4f9fdb8dbf7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### function to organise traj data to call plotter function ‚®èüó∫üìç\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c7f41bd9-a5f0-4c42-90be-dca3c37ce336",
   "metadata": {},
   "outputs": [],
   "source": [
    "def organise_indv_traj_plot(all_data, outputdir):\n",
    "    # set some constants\n",
    "    min_x = 0\n",
    "    max_x = 46000\n",
    "    min_y = 0\n",
    "    max_y = 46000\n",
    "    fig, ax = plt.subplots()\n",
    "    # cycle through all worms\n",
    "    for worm in wormlist:\n",
    "        this_worm = all_data[all_data[\"unique_id\"] == worm]\n",
    "        cond = all_data[\"cond\"][all_data[\"unique_id\"] == worm].unique().tolist()[0]\n",
    "        wormstr = np.array2string(worm)\n",
    "        thisx = this_worm[\"coord_x_body\"]\n",
    "        thisy = this_worm[\"coord_y_body\"]\n",
    "        thisframe = this_worm[\"frame_number\"]\n",
    "        this_filename = outputdir + cond + wormstr\n",
    "\n",
    "        this_cmap = \"viridis\"\n",
    "        # now plot trajectory of this worm using above variables\n",
    "\n",
    "        plot_traj(\n",
    "            thisx,\n",
    "            thisy,\n",
    "            thisframe,\n",
    "            this_cmap,\n",
    "            min_x,\n",
    "            max_x,\n",
    "            min_y,\n",
    "            max_y,\n",
    "            this_filename,\n",
    "        )\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790fceab-3219-45bb-bbf2-dfb35cf30124",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### function to make pretty trajectory plots ‚®èüé®üìç"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6e02f220-0066-4cf5-9cc9-42f5b8948b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_traj(xvals, yvals, frames, this_cmap, minx, maxx, miny, maxy, filename):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.scatter(xvals, yvals, s=0.1, c=frames, cmap=this_cmap, marker=\".\")\n",
    "    ax.set_xlim(left=minx, right=maxx)\n",
    "    ax.set_ylim(bottom=miny, top=maxy)\n",
    "\n",
    "    plt.savefig(\n",
    "        filename,\n",
    "        transparent=True,\n",
    "    )\n",
    "    plt.show()\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b5aca7a9-0123-4535-a950-58da7ce2a295",
   "metadata": {},
   "outputs": [],
   "source": [
    "def organise_indv_traj_plot(true_data, outputdir):\n",
    "    # set some constants\n",
    "    min_x = 0\n",
    "    max_x = 46000\n",
    "    min_y = 0\n",
    "    max_y = 46000\n",
    "    fig, ax = plt.subplots()\n",
    "    # cycle through true worms\n",
    "    for worm in wormlist:\n",
    "        this_worm = true_data[true_data[\"unique_id\"] == worm]\n",
    "        cond = true_data[\"cond\"][true_data[\"unique_id\"] == worm].unique().tolist()[0]\n",
    "        wormstr = np.array2string(worm)\n",
    "        thisx = this_worm[\"coord_x_body\"]\n",
    "        thisy = this_worm[\"coord_y_body\"]\n",
    "        thisframe = this_worm[\"frame_number\"]\n",
    "        this_filename = outputdir + cond + wormstr\n",
    "\n",
    "        this_cmap = \"viridis\"\n",
    "        # now plot trajectory of this worm using above variables\n",
    "\n",
    "        plot_traj(\n",
    "            thisx,\n",
    "            thisy,\n",
    "            thisframe,\n",
    "            this_cmap,\n",
    "            min_x,\n",
    "            max_x,\n",
    "            min_y,\n",
    "            max_y,\n",
    "            this_filename,\n",
    "        )\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53dfe74-38b4-4df7-9ef6-fde33f1bddd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5b80a4a8-1ea4-429e-8da8-681b94f0cd63",
   "metadata": {},
   "source": [
    "## Inputs ü•öüßàü´ê\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "56d8926b-c730-471f-939f-cd31b92c3ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## only inputs needed for the whole thingymawhatsits to run\n",
    "\n",
    "#####################################################################\n",
    "\n",
    "\n",
    "# 1. set output directory (need to make it match name of legit existing folder)\n",
    "outputdirhim5 = \"/home/Documents/neuroUCL/phd/current/project/tierpsy stuff for tracking and reversals/duringconditioning/him-5/feb2/\"\n",
    "outputdirBAR302 = \"/home/Documents/neuroUCL/phd/current/project/tierpsy stuff for tracking and reversals/BAR302/11jan/\"\n",
    "\n",
    "# 2. set code to choose true other parameters and suffix (HIM-5 / BAR302)\n",
    "code = \"HIM-5\"\n",
    "# code = \"BAR302\"\n",
    "\n",
    "\n",
    "####################################################################\n",
    "\n",
    "\n",
    "if code == \"HIM-5\":\n",
    "    outputdir = outputdirhim5\n",
    "\n",
    "    folders = [\n",
    "        \"/home/Documents/neuroUCL/phd/current/project/tierpsy stuff for tracking and reversals/data_duringcond/Results/mock\",\n",
    "        \"/home/Documents/neuroUCL/phd/current/project/tierpsy stuff for tracking and reversals/data_duringcond/Results/avsv\",\n",
    "    ]\n",
    "\n",
    "elif code == \"BAR302\":\n",
    "    outputdir = outputdirBAR302\n",
    "\n",
    "    # folders = [\n",
    "    #     \"/home/Documents/neuroUCL/phd/current/project/tierpsy stuff for tracking and reversals/data_uptoDEC/Results/BAR302_aftercond/mock\",\n",
    "    #     \"/home/Documents/neuroUCL/phd/current/project/tierpsy stuff for tracking and reversals/data_uptoDEC/Results/BAR302_aftercond/avsv\",\n",
    "    #     \"/home/Documents/neuroUCL/phd/current/project/tierpsy stuff for tracking and reversals/data_uptoDEC/Results/BAR302_aftercond/sexc\",\n",
    "    # ]\n",
    "\n",
    "else:\n",
    "    print(\"error in input code for condition. should be HIM-5 or BAR302\")\n",
    "\n",
    "\n",
    "##true other parameters are here\n",
    "\n",
    "# enter framerate (15fps)\n",
    "framerate = 15\n",
    "\n",
    "# max frames missing - max frames that can be missing inside rev and still consider it one reversal instead of 2 (also for forward etc)\n",
    "# should be 1 sec and is 15fps so 15 frames\n",
    "max_frames_missing = 15\n",
    "\n",
    "# min_good_frames_percent - maximum frames a worm can be missing to be considered for analysis - e.g. if is 30 percent worms that have less than 30% good frames excluded\n",
    "min_good_frames_percent = 30\n",
    "\n",
    "# min and max length for considering as reversals\n",
    "minrevlen = 15\n",
    "maxrevlen = 80\n",
    "\n",
    "# min and max omega length\n",
    "minomegalength = 45\n",
    "maxomegalength = 400"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055b96f3-2f06-41a4-b387-7d240e227b10",
   "metadata": {},
   "source": [
    "## Actual code to call functions is here üìû"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8dd87cbc-409c-49cb-8b59-2b279db2bd38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_11469/2802986360.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  bigdict[pathname_dict[key][i]][\"data\"][\"cond\"] = key\n",
      "/tmp/ipykernel_11469/2802986360.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  bigdict[pathname_dict[key][i]][\"data\"][\"filename\"] = pathname_dict[key][i]\n",
      "/tmp/ipykernel_11469/2802986360.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  bigdict[pathname_dict[key][i]][\"data\"][\"cond\"] = key\n",
      "/tmp/ipykernel_11469/2802986360.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  bigdict[pathname_dict[key][i]][\"data\"][\"filename\"] = pathname_dict[key][i]\n",
      "/tmp/ipykernel_11469/2802986360.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  bigdict[pathname_dict[key][i]][\"data\"][\"cond\"] = key\n",
      "/tmp/ipykernel_11469/2802986360.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  bigdict[pathname_dict[key][i]][\"data\"][\"filename\"] = pathname_dict[key][i]\n",
      "/tmp/ipykernel_11469/2802986360.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  bigdict[pathname_dict[key][i]][\"data\"][\"cond\"] = key\n",
      "/tmp/ipykernel_11469/2802986360.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  bigdict[pathname_dict[key][i]][\"data\"][\"filename\"] = pathname_dict[key][i]\n",
      "/tmp/ipykernel_11469/2802986360.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  bigdict[pathname_dict[key][i]][\"data\"][\"cond\"] = key\n",
      "/tmp/ipykernel_11469/2802986360.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  bigdict[pathname_dict[key][i]][\"data\"][\"filename\"] = pathname_dict[key][i]\n",
      "/tmp/ipykernel_11469/2802986360.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  bigdict[pathname_dict[key][i]][\"data\"][\"cond\"] = key\n",
      "/tmp/ipykernel_11469/2802986360.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  bigdict[pathname_dict[key][i]][\"data\"][\"filename\"] = pathname_dict[key][i]\n",
      "/tmp/ipykernel_11469/2802986360.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  bigdict[pathname_dict[key][i]][\"data\"][\"cond\"] = key\n",
      "/tmp/ipykernel_11469/2802986360.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  bigdict[pathname_dict[key][i]][\"data\"][\"filename\"] = pathname_dict[key][i]\n",
      "/tmp/ipykernel_11469/2802986360.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  bigdict[pathname_dict[key][i]][\"data\"][\"cond\"] = key\n",
      "/tmp/ipykernel_11469/2802986360.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  bigdict[pathname_dict[key][i]][\"data\"][\"filename\"] = pathname_dict[key][i]\n",
      "/tmp/ipykernel_11469/2802986360.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  bigdict[pathname_dict[key][i]][\"data\"][\"cond\"] = key\n",
      "/tmp/ipykernel_11469/2802986360.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  bigdict[pathname_dict[key][i]][\"data\"][\"filename\"] = pathname_dict[key][i]\n",
      "/tmp/ipykernel_11469/2802986360.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  bigdict[pathname_dict[key][i]][\"data\"][\"cond\"] = key\n",
      "/tmp/ipykernel_11469/2802986360.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  bigdict[pathname_dict[key][i]][\"data\"][\"filename\"] = pathname_dict[key][i]\n",
      "/tmp/ipykernel_11469/2802986360.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  bigdict[pathname_dict[key][i]][\"data\"][\"cond\"] = key\n",
      "/tmp/ipykernel_11469/2802986360.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  bigdict[pathname_dict[key][i]][\"data\"][\"filename\"] = pathname_dict[key][i]\n",
      "/tmp/ipykernel_11469/2802986360.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  bigdict[pathname_dict[key][i]][\"data\"][\"cond\"] = key\n",
      "/tmp/ipykernel_11469/2802986360.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  bigdict[pathname_dict[key][i]][\"data\"][\"filename\"] = pathname_dict[key][i]\n",
      "/tmp/ipykernel_11469/2802986360.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  bigdict[pathname_dict[key][i]][\"data\"][\"cond\"] = key\n",
      "/tmp/ipykernel_11469/2802986360.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  bigdict[pathname_dict[key][i]][\"data\"][\"filename\"] = pathname_dict[key][i]\n",
      "/tmp/ipykernel_11469/2802986360.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  bigdict[pathname_dict[key][i]][\"data\"][\"cond\"] = key\n",
      "/tmp/ipykernel_11469/2802986360.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  bigdict[pathname_dict[key][i]][\"data\"][\"filename\"] = pathname_dict[key][i]\n",
      "/tmp/ipykernel_11469/2802986360.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  bigdict[pathname_dict[key][i]][\"data\"][\"cond\"] = key\n",
      "/tmp/ipykernel_11469/2802986360.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  bigdict[pathname_dict[key][i]][\"data\"][\"filename\"] = pathname_dict[key][i]\n",
      "/tmp/ipykernel_11469/2802986360.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  bigdict[pathname_dict[key][i]][\"data\"][\"cond\"] = key\n",
      "/tmp/ipykernel_11469/2802986360.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  bigdict[pathname_dict[key][i]][\"data\"][\"filename\"] = pathname_dict[key][i]\n",
      "/tmp/ipykernel_11469/2802986360.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  bigdict[pathname_dict[key][i]][\"data\"][\"cond\"] = key\n",
      "/tmp/ipykernel_11469/2802986360.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  bigdict[pathname_dict[key][i]][\"data\"][\"filename\"] = pathname_dict[key][i]\n",
      "/tmp/ipykernel_11469/434750914.py:20: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_df[\"unique_id_long\"] = data_df[\"filename\"] + data_df[\"worm_id\"].astype(str)\n"
     ]
    }
   ],
   "source": [
    "# set some variables\n",
    "conds = [\"mock\", \"avsv\", \"sexc\"]  # condition codes to append as needed\n",
    "features_filename = \"metadata_featuresN.hdf5\"\n",
    "\n",
    "pathnames_dict = dict()\n",
    "# loop to get a dictionary containing the  pathnames for each condition\n",
    "for f in range(0, len(folders)):\n",
    "    this_path_list = get_file_paths(folders[f], features_filename)\n",
    "    pathnames_dict[conds[f]] = this_path_list\n",
    "\n",
    "del f\n",
    "del this_path_list\n",
    "\n",
    "\n",
    "# now have pathnames dict, need to reorganise data and keep only the bits we want\n",
    "data_df, data_dict = organise_data(pathnames_dict)\n",
    "\n",
    "\n",
    "# create unique ids for each worm and save xls with equivalency to worm video (uniquelongid)\n",
    "data_df[\"unique_id_long\"] = data_df[\"filename\"] + data_df[\"worm_id\"].astype(str)\n",
    "newids = data_df[\"unique_id_long\"].unique()\n",
    "df_newids = pd.Series(newids, name=\"unique_id_long\").reset_index()\n",
    "df_newids = df_newids.rename(columns={\"index\": \"unique_id\"})\n",
    "data_df = data_df.merge(df_newids, on=\"unique_id_long\")\n",
    "df_newids.to_excel(outputdir + \"worm_unique_id_\" + code + \".xlsx\")\n",
    "\n",
    "\n",
    "###################################################\n",
    "\n",
    "# FOR NOW DONT RUN THIS CAUSE LOOSE LOTS OF WORMS\n",
    "# # ctrue function to remove worms with few frames\n",
    "# sheared_data_df = remove_shitty_worms(data_df)\n",
    "# true_worms_data_df = data_df.copy()\n",
    "# data_df = sheared_data_df.copy()\n",
    "# del sheared_data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd9821a-5e79-43cb-9064-c5757514900d",
   "metadata": {},
   "source": [
    "### call  find/clean reversals, forward runs, and omegas functionsüìû‚Ü©Ô∏è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "340529cb-8bce-4e7a-b805-a5ebba910055",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_11469/2930054385.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_df[\"RevStart\"] = False\n",
      "/tmp/ipykernel_11469/2930054385.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_df[\"RevEnd\"] = False\n",
      "/tmp/ipykernel_11469/2930054385.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_df[\"FwdStart\"] = False\n",
      "/tmp/ipykernel_11469/2930054385.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_df[\"FwdEnd\"] = False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this worm:  0\n",
      "this worm:  1\n",
      "this worm:  2\n",
      "this worm:  3\n",
      "this worm:  4\n",
      "this worm:  5\n",
      "this worm:  6\n",
      "this worm:  7\n",
      "this worm:  8\n",
      "this worm:  9\n",
      "this worm:  10\n",
      "this worm:  11\n",
      "this worm:  12\n",
      "this worm:  13\n",
      "this worm:  14\n",
      "this worm:  15\n",
      "this worm:  16\n",
      "this worm:  17\n",
      "this worm:  18\n",
      "this worm:  19\n",
      "this worm:  20\n",
      "this worm:  21\n",
      "this worm:  22\n",
      "this worm:  23\n",
      "this worm:  24\n",
      "this worm:  25\n",
      "this worm:  26\n",
      "this worm:  27\n"
     ]
    }
   ],
   "source": [
    "# to run reversals AND forward run functions\n",
    "# create list with unique wormlist\n",
    "wormlist = data_df[\"unique_id\"].unique()\n",
    "\n",
    "\n",
    "# will contain rev len lists and unique id list matching length to create all_rev_lengths\n",
    "revlengs_ids = []\n",
    "revlengs = []\n",
    "\n",
    "# will contain fwd run len lists and unique id list matching length to create true_fwd_lengths\n",
    "fwdlengs_ids = []\n",
    "fwdlengs = []\n",
    "\n",
    "\n",
    "# add columns to contain fwd and rev start and end indices\n",
    "data_df[\"RevStart\"] = False\n",
    "data_df[\"RevEnd\"] = False\n",
    "data_df[\"FwdStart\"] = False\n",
    "data_df[\"FwdEnd\"] = False\n",
    "\n",
    "\n",
    "for worm in wormlist:\n",
    "    print(\"this worm: \", worm)\n",
    "    this_worm_speed = data_df[[\"frame_number\", \"speed\"]][data_df[\"unique_id\"] == worm]\n",
    "\n",
    "    # find and clean reversals\n",
    "    this_worm_RS_dirty, this_worm_RE_dirty = findreversals(this_worm_speed)\n",
    "    this_worm_RS, this_worm_RE, this_worm_RLg = cleanreversals(\n",
    "        this_worm_speed, this_worm_RS_dirty, this_worm_RE_dirty\n",
    "    )\n",
    "\n",
    "    # find and clean fwd runs\n",
    "    this_worm_FS_dirty, this_worm_FE_dirty = findfwdruns(this_worm_speed)\n",
    "    this_worm_FS, this_worm_FE, this_worm_FLg = cleanfwdruns(\n",
    "        this_worm_speed, this_worm_FS_dirty, this_worm_FE_dirty\n",
    "    )\n",
    "\n",
    "    # to store reversal and fwd run lengths\n",
    "    # cycle thrugh list of reversal/fwdrun lengths and append worm id to worm id list and rev/fwd leng to rev/fwd list\n",
    "    # then will join both these lists into df\n",
    "    for k in this_worm_RLg:\n",
    "        revlengs_ids.append(worm)\n",
    "        revlengs.append(k)\n",
    "\n",
    "    for m in this_worm_FLg:\n",
    "        fwdlengs_ids.append(worm)\n",
    "        fwdlengs.append(m)\n",
    "\n",
    "    # now also create columns in data_df that contain true at reversal start/end points (clean)\n",
    "    data_df.loc[this_worm_RS, \"RevStart\"] = True\n",
    "    data_df.loc[this_worm_RE, \"RevEnd\"] = True\n",
    "    data_df.loc[this_worm_FS, \"FwdStart\"] = True\n",
    "    data_df.loc[this_worm_FE, \"FwdEnd\"] = True\n",
    "\n",
    "# thisdf will contain unique worm id (short) as 1st column and the length of each reversal in 2nd column, another df same for fwdruns\n",
    "all_rev_lengths = pd.DataFrame()\n",
    "all_rev_lengths = all_rev_lengths.assign(unique_id=revlengs_ids, revlen=revlengs)\n",
    "\n",
    "all_fwd_lengths = pd.DataFrame()\n",
    "all_fwd_lengths = all_fwd_lengths.assign(unique_id=fwdlengs_ids, fwdlen=fwdlengs)\n",
    "\n",
    "\n",
    "# now get rid of ONLY REVERSALS that are too long or too short (dont apply this to forward runs!)\n",
    "# then rename the the old dataframe so still have access, but use new_data_df as data_df\n",
    "\n",
    "new_data_df, true_rev_lengths = exclude_short_long_revs(data_df)\n",
    "data_df_all_revs = data_df.copy()\n",
    "del data_df\n",
    "data_df = new_data_df.copy()\n",
    "del new_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d3e79317-cfbd-4871-8d79-0cfda10569a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add is fwd or is bck column to data_df\n",
    "data_df[\"fw\"] = False\n",
    "data_df[\"bk\"] = False\n",
    "\n",
    "for row in range(len(data_df)):\n",
    "    if data_df.loc[row, \"speed\"] > 0:\n",
    "        data_df.loc[row, \"fw\"] = True\n",
    "    elif data_df.loc[row, \"speed\"] < 0:\n",
    "        data_df.loc[row, \"bk\"] = True\n",
    "\n",
    "\n",
    "# count good frames\n",
    "wormlist = data_df[\"unique_id\"].unique()\n",
    "gf_per_worm = count_good_frames(data_df)\n",
    "\n",
    "gf_per_worm\n",
    "\n",
    "\n",
    "# cal function to calculate percent time fwd and back\n",
    "fwdbck = calculate_fbpercent(gf_per_worm, data_df)\n",
    "fwdbck\n",
    "\n",
    "fwdbck.to_excel(outputdir + \"fwdbck\" + code + \".xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f67333d-0bd9-4cc2-a046-74fd274d1f41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952cbc4a-c8c7-4554-a73c-be73f0af330c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "27998ba7-f960-43f4-9a8b-f555a3e8c867",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>worm_id</th>\n",
       "      <th>has_skeleton</th>\n",
       "      <th>frame_number</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>speed</th>\n",
       "      <th>angular_velocity</th>\n",
       "      <th>relative_to_body_speed_midbody</th>\n",
       "      <th>relative_to_body_radial_velocity_head_tip</th>\n",
       "      <th>relative_to_body_angular_velocity_head_tip</th>\n",
       "      <th>...</th>\n",
       "      <th>unique_id_long</th>\n",
       "      <th>unique_id</th>\n",
       "      <th>ogRevStart</th>\n",
       "      <th>ogRevEnd</th>\n",
       "      <th>FwdStart</th>\n",
       "      <th>FwdEnd</th>\n",
       "      <th>RevStart</th>\n",
       "      <th>RevEnd</th>\n",
       "      <th>fw</th>\n",
       "      <th>bk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>65534</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1712.0</td>\n",
       "      <td>2357.853516</td>\n",
       "      <td>1609.601440</td>\n",
       "      <td>-8.048562</td>\n",
       "      <td>0.093520</td>\n",
       "      <td>-10.875153</td>\n",
       "      <td>-72.958847</td>\n",
       "      <td>-0.247894</td>\n",
       "      <td>...</td>\n",
       "      <td>/home/Documents/neuroUCL/phd/current/project/t...</td>\n",
       "      <td>19</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65613</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1791.0</td>\n",
       "      <td>2353.927734</td>\n",
       "      <td>1602.172363</td>\n",
       "      <td>-22.600433</td>\n",
       "      <td>0.225060</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>/home/Documents/neuroUCL/phd/current/project/t...</td>\n",
       "      <td>19</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65663</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1841.0</td>\n",
       "      <td>2350.437744</td>\n",
       "      <td>1617.131348</td>\n",
       "      <td>-9.490545</td>\n",
       "      <td>0.196564</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>/home/Documents/neuroUCL/phd/current/project/t...</td>\n",
       "      <td>19</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65680</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1858.0</td>\n",
       "      <td>2352.784668</td>\n",
       "      <td>1610.534424</td>\n",
       "      <td>-97.581970</td>\n",
       "      <td>0.169504</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>/home/Documents/neuroUCL/phd/current/project/t...</td>\n",
       "      <td>19</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66059</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2237.0</td>\n",
       "      <td>2354.351318</td>\n",
       "      <td>1552.182007</td>\n",
       "      <td>-22.771564</td>\n",
       "      <td>0.013324</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>/home/Documents/neuroUCL/phd/current/project/t...</td>\n",
       "      <td>19</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72715</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8893.0</td>\n",
       "      <td>2744.925293</td>\n",
       "      <td>1493.345947</td>\n",
       "      <td>-15.809999</td>\n",
       "      <td>-0.032908</td>\n",
       "      <td>3.601770</td>\n",
       "      <td>-7.211644</td>\n",
       "      <td>-0.061333</td>\n",
       "      <td>...</td>\n",
       "      <td>/home/Documents/neuroUCL/phd/current/project/t...</td>\n",
       "      <td>19</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72755</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8933.0</td>\n",
       "      <td>2742.832520</td>\n",
       "      <td>1486.052368</td>\n",
       "      <td>-1.233408</td>\n",
       "      <td>-0.003970</td>\n",
       "      <td>-0.551683</td>\n",
       "      <td>1.916762</td>\n",
       "      <td>0.001126</td>\n",
       "      <td>...</td>\n",
       "      <td>/home/Documents/neuroUCL/phd/current/project/t...</td>\n",
       "      <td>19</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72804</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8982.0</td>\n",
       "      <td>2745.799072</td>\n",
       "      <td>1491.856445</td>\n",
       "      <td>-8.543286</td>\n",
       "      <td>0.012822</td>\n",
       "      <td>1.328621</td>\n",
       "      <td>-14.043110</td>\n",
       "      <td>0.127997</td>\n",
       "      <td>...</td>\n",
       "      <td>/home/Documents/neuroUCL/phd/current/project/t...</td>\n",
       "      <td>19</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72809</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8987.0</td>\n",
       "      <td>2745.691162</td>\n",
       "      <td>1491.999268</td>\n",
       "      <td>-1.330577</td>\n",
       "      <td>-0.020607</td>\n",
       "      <td>-1.217804</td>\n",
       "      <td>0.332716</td>\n",
       "      <td>-0.016660</td>\n",
       "      <td>...</td>\n",
       "      <td>/home/Documents/neuroUCL/phd/current/project/t...</td>\n",
       "      <td>19</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72816</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8994.0</td>\n",
       "      <td>2746.076660</td>\n",
       "      <td>1493.193481</td>\n",
       "      <td>-1.751623</td>\n",
       "      <td>-0.044231</td>\n",
       "      <td>-0.022304</td>\n",
       "      <td>-8.369317</td>\n",
       "      <td>-0.301749</td>\n",
       "      <td>...</td>\n",
       "      <td>/home/Documents/neuroUCL/phd/current/project/t...</td>\n",
       "      <td>19</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>107 rows √ó 162 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       worm_id  has_skeleton  frame_number            x            y  \\\n",
       "65534      1.0           1.0        1712.0  2357.853516  1609.601440   \n",
       "65613      1.0           1.0        1791.0  2353.927734  1602.172363   \n",
       "65663      1.0           1.0        1841.0  2350.437744  1617.131348   \n",
       "65680      1.0           1.0        1858.0  2352.784668  1610.534424   \n",
       "66059      1.0           1.0        2237.0  2354.351318  1552.182007   \n",
       "...        ...           ...           ...          ...          ...   \n",
       "72715      1.0           1.0        8893.0  2744.925293  1493.345947   \n",
       "72755      1.0           1.0        8933.0  2742.832520  1486.052368   \n",
       "72804      1.0           1.0        8982.0  2745.799072  1491.856445   \n",
       "72809      1.0           1.0        8987.0  2745.691162  1491.999268   \n",
       "72816      1.0           1.0        8994.0  2746.076660  1493.193481   \n",
       "\n",
       "           speed  angular_velocity  relative_to_body_speed_midbody  \\\n",
       "65534  -8.048562          0.093520                      -10.875153   \n",
       "65613 -22.600433          0.225060                             NaN   \n",
       "65663  -9.490545          0.196564                             NaN   \n",
       "65680 -97.581970          0.169504                             NaN   \n",
       "66059 -22.771564          0.013324                             NaN   \n",
       "...          ...               ...                             ...   \n",
       "72715 -15.809999         -0.032908                        3.601770   \n",
       "72755  -1.233408         -0.003970                       -0.551683   \n",
       "72804  -8.543286          0.012822                        1.328621   \n",
       "72809  -1.330577         -0.020607                       -1.217804   \n",
       "72816  -1.751623         -0.044231                       -0.022304   \n",
       "\n",
       "       relative_to_body_radial_velocity_head_tip  \\\n",
       "65534                                 -72.958847   \n",
       "65613                                        NaN   \n",
       "65663                                        NaN   \n",
       "65680                                        NaN   \n",
       "66059                                        NaN   \n",
       "...                                          ...   \n",
       "72715                                  -7.211644   \n",
       "72755                                   1.916762   \n",
       "72804                                 -14.043110   \n",
       "72809                                   0.332716   \n",
       "72816                                  -8.369317   \n",
       "\n",
       "       relative_to_body_angular_velocity_head_tip  ...  \\\n",
       "65534                                   -0.247894  ...   \n",
       "65613                                         NaN  ...   \n",
       "65663                                         NaN  ...   \n",
       "65680                                         NaN  ...   \n",
       "66059                                         NaN  ...   \n",
       "...                                           ...  ...   \n",
       "72715                                   -0.061333  ...   \n",
       "72755                                    0.001126  ...   \n",
       "72804                                    0.127997  ...   \n",
       "72809                                   -0.016660  ...   \n",
       "72816                                   -0.301749  ...   \n",
       "\n",
       "                                          unique_id_long  unique_id  \\\n",
       "65534  /home/Documents/neuroUCL/phd/current/project/t...         19   \n",
       "65613  /home/Documents/neuroUCL/phd/current/project/t...         19   \n",
       "65663  /home/Documents/neuroUCL/phd/current/project/t...         19   \n",
       "65680  /home/Documents/neuroUCL/phd/current/project/t...         19   \n",
       "66059  /home/Documents/neuroUCL/phd/current/project/t...         19   \n",
       "...                                                  ...        ...   \n",
       "72715  /home/Documents/neuroUCL/phd/current/project/t...         19   \n",
       "72755  /home/Documents/neuroUCL/phd/current/project/t...         19   \n",
       "72804  /home/Documents/neuroUCL/phd/current/project/t...         19   \n",
       "72809  /home/Documents/neuroUCL/phd/current/project/t...         19   \n",
       "72816  /home/Documents/neuroUCL/phd/current/project/t...         19   \n",
       "\n",
       "       ogRevStart  ogRevEnd  FwdStart  FwdEnd  RevStart  RevEnd     fw    bk  \n",
       "65534        True     False     False   False     False   False  False  True  \n",
       "65613        True     False     False   False      True   False  False  True  \n",
       "65663        True      True     False   False     False   False  False  True  \n",
       "65680        True     False     False   False      True   False  False  True  \n",
       "66059        True     False     False   False      True   False  False  True  \n",
       "...           ...       ...       ...     ...       ...     ...    ...   ...  \n",
       "72715        True     False     False   False      True   False  False  True  \n",
       "72755        True     False     False   False     False   False  False  True  \n",
       "72804        True     False     False   False     False   False  False  True  \n",
       "72809        True     False     False   False     False   False  False  True  \n",
       "72816        True     False     False   False     False   False  False  True  \n",
       "\n",
       "[107 rows x 162 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df[(data_df[\"unique_id\"] == 19) & (data_df[\"ogRevStart\"] == True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6eec7bbc-633f-42b1-acb4-d1fe2426fb61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>revlen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>19</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>19</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>19</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>19</td>\n",
       "      <td>35.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>19</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323</th>\n",
       "      <td>19</td>\n",
       "      <td>37.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>324</th>\n",
       "      <td>19</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325</th>\n",
       "      <td>19</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326</th>\n",
       "      <td>19</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>327</th>\n",
       "      <td>19</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>107 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     unique_id  revlen\n",
       "221         19     2.0\n",
       "222         19    21.0\n",
       "223         19     1.0\n",
       "224         19    35.0\n",
       "225         19    17.0\n",
       "..         ...     ...\n",
       "323         19    37.0\n",
       "324         19     4.0\n",
       "325         19     2.0\n",
       "326         19     2.0\n",
       "327         19     3.0\n",
       "\n",
       "[107 rows x 2 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_rev_lengths[all_rev_lengths[\"unique_id\"] == 19]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2b5cce6e-3568-4dbb-bf8b-b285b675b709",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1586.0"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(all_rev_lengths[\"revlen\"][all_rev_lengths[\"unique_id\"] == 19])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
